{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Anusha_Ayyagari_data602_week12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNSsgwMQY7i-",
        "outputId": "d2d293a6-8596-47d0-be78-f74dc4f7547c"
      },
      "source": [
        "#Install preprocessing with Keras\n",
        "\n",
        "! pip install Keras\n",
        "! pip install Keras-Preprocessing\n",
        "! pip install keras.processing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from Keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->Keras) (1.15.0)\n",
            "Requirement already satisfied: Keras-Preprocessing in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras-Preprocessing) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from Keras-Preprocessing) (1.19.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras.processing (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for keras.processing\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYpbHhoTaF5i"
      },
      "source": [
        "#Load all the necessary libraries\n",
        "#Load tensorflow.keras.models\n",
        "#Load the dataset using from keras.datasets import imdb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import imdb\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJAocm3mdMJH",
        "outputId": "3560a719-2a2b-45d3-dfbb-493112da3e9c"
      },
      "source": [
        "#code refernce https://keras.io/api/datasets/imdb\n",
        "#since the ouput for the imdb.load_data is x_train, x_test and y_train, y_test\n",
        "# warnings for VisibleDeprecationWarning resolved via https://stackoverflow.com/questions/66414449/importing-imdb-dataset-from-keras-dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxGeYmKRES01",
        "outputId": "53345d04-df06-4b5d-cd88-cece2b672039"
      },
      "source": [
        "#Print the number of unique categories for the target variable\n",
        "#Print the average review length\n",
        "#Print the standard deviation\n",
        "# for all the three questions it has not beeen mentioned that this has to be test data or training data so i am exploring this on the training data\n",
        "\n",
        "#print(type(X_train))\n",
        "# code refernce https://numpy.org/doc/stable/reference/generated/numpy.unique.html\n",
        "\n",
        "a=np.unique(y_train)\n",
        "print('the unique categories are',a)\n",
        "\n",
        "#new_lst=[]\n",
        "#for i in X_train:\n",
        " # new_lst.append(len(i))\n",
        "      \n",
        "#print(len(new_lst))\n",
        "#new_train=np.array(new_lst)\n",
        "\n",
        "#using numpy operations\n",
        "#code refence https://www.geeksforgeeks.org/compute-the-mean-standard-deviation-and-variance-of-a-given-numpy-array/\n",
        "movie_len=[len(i) for i in X_train]\n",
        "avg_mv_len=np.mean(movie_len)\n",
        "print('the average lenght of the movies is',avg_mv_len)\n",
        "std_mov=np.std(movie_len)\n",
        "print('the standard devistion of the of the movies dataset is',std_mov)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the unique categories are [0 1]\n",
            "the average lenght of the movies is 238.71364\n",
            "the standard devistion of the of the movies dataset is 176.49367364852034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1H6TEJ-hoAK",
        "outputId": "3e1430cf-0288-46dd-ff08-b37d799af220"
      },
      "source": [
        "#Vectorize sequences and dimension with a size of 10,000\n",
        "#code refernce https://www.javaer101.com/en/article/18836089.html\n",
        "#https://www.geeksforgeeks.org/vectorization-in-python/\n",
        "#https://colab.research.google.com/drive/1GSsIKJ8Uqy2Qi6TOBPlWvCSZmMLIlhh6\n",
        "\n",
        "def vector_sequence(data, dimension=10000):\n",
        "    vector = np.zeros((len(data), dimension))\n",
        "    for i, data in enumerate(data):\n",
        "        vector[i,data] = 1.\n",
        "    return vector\n",
        "\n",
        "train_data = vector_sequence(X_train)\n",
        "\n",
        "print(train_data.shape)\n",
        "\n",
        "test_data = vector_sequence(X_test)\n",
        "\n",
        "print(test_data.shape)\n",
        "\n",
        "#Make sure the test and train variables represent 10,000 observations\n",
        "\n",
        "print('number of of observations in the test data',len(test_data[0]))\n",
        "print('number of of observations in the train data',len(train_data[0]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 10000)\n",
            "(25000, 10000)\n",
            "number of of observations in the test data 10000\n",
            "number of of observations in the train data 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv2spzWX1Fyk",
        "outputId": "89a07e15-67f2-49d8-bd2c-38c73771d225"
      },
      "source": [
        "#Import Sequential, Flatten, Dropout, and Dense from tensorflow.keras.models\n",
        "# Flatten, Dropout, and Dense  are in the tensorflow.keras.layers not the tensorflow.keras.models reference https://stackoverflow.com/questions/59388162/importerror-cannot-import-name-sequential-from-keras-models\n",
        "import tensorflow.keras.models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "\n",
        "#code refernce for dropout from https://keras.io/api/layers/regularization_layers/dropout/ i am setting  all of them at 0.5 and putting all the default parameters from the defination in the keras library\n",
        "model = Sequential()\n",
        "# code refrence for input shape https://keras.io/api/layers/core_layers/dense/ each row has a batch of 10000 obeservations\n",
        "model.add(Dense(50, activation = \"relu\",input_shape=(10000, )))\n",
        "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(Dense(1024, activation = \"relu\"))\n",
        "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(Dense(2, activation = \"relu\"))\n",
        "model.add(Dense(1, activation = \"relu\"))\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              52224     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 2050      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 554,327\n",
            "Trainable params: 554,327\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVmkUXm93tG",
        "outputId": "94af84df-6877-4772-a562-9efb2bfaf43e"
      },
      "source": [
        "#Use the following parameters with an ‘adam’ optimizer and ‘binary_crossentropy’ loss function\n",
        "#code refernce https://colab.research.google.com/drive/1B6TLosBa-q9d9knwlRkvGokv2W2MGtsH\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "#Compile, train, and evaluate the model with epochs=20, batch_size=512, validation_data=(test_x,test_y)\n",
        "model.fit(train_data,y_train,epochs=20,batch_size=512,validation_data=(test_data,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 1.4603 - accuracy: 0.5636 - val_loss: 0.4487 - val_accuracy: 0.8439\n",
            "Epoch 2/20\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.3563 - accuracy: 0.8805 - val_loss: 0.3747 - val_accuracy: 0.8725\n",
            "Epoch 3/20\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.2442 - accuracy: 0.9347 - val_loss: 0.4102 - val_accuracy: 0.8630\n",
            "Epoch 4/20\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.1910 - accuracy: 0.9441 - val_loss: 0.5272 - val_accuracy: 0.8689\n",
            "Epoch 5/20\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.1451 - accuracy: 0.9693 - val_loss: 0.5758 - val_accuracy: 0.8687\n",
            "Epoch 6/20\n",
            "49/49 [==============================] - 4s 79ms/step - loss: 0.1188 - accuracy: 0.9819 - val_loss: 0.6142 - val_accuracy: 0.8616\n",
            "Epoch 7/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0971 - accuracy: 0.9857 - val_loss: 0.7092 - val_accuracy: 0.8630\n",
            "Epoch 8/20\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.0765 - accuracy: 0.9914 - val_loss: 0.7526 - val_accuracy: 0.8661\n",
            "Epoch 9/20\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.0661 - accuracy: 0.9938 - val_loss: 0.7867 - val_accuracy: 0.8670\n",
            "Epoch 10/20\n",
            "49/49 [==============================] - 4s 76ms/step - loss: 0.0609 - accuracy: 0.9947 - val_loss: 0.8302 - val_accuracy: 0.8670\n",
            "Epoch 11/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0680 - accuracy: 0.9947 - val_loss: 0.8728 - val_accuracy: 0.8667\n",
            "Epoch 12/20\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.0615 - accuracy: 0.9952 - val_loss: 0.9018 - val_accuracy: 0.8668\n",
            "Epoch 13/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0576 - accuracy: 0.9956 - val_loss: 0.9125 - val_accuracy: 0.8670\n",
            "Epoch 14/20\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.0599 - accuracy: 0.9954 - val_loss: 0.8904 - val_accuracy: 0.8658\n",
            "Epoch 15/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0754 - accuracy: 0.9947 - val_loss: 0.8948 - val_accuracy: 0.8658\n",
            "Epoch 16/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0690 - accuracy: 0.9950 - val_loss: 0.8962 - val_accuracy: 0.8659\n",
            "Epoch 17/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0631 - accuracy: 0.9950 - val_loss: 0.9342 - val_accuracy: 0.8654\n",
            "Epoch 18/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0615 - accuracy: 0.9957 - val_loss: 0.9598 - val_accuracy: 0.8648\n",
            "Epoch 19/20\n",
            "49/49 [==============================] - 4s 77ms/step - loss: 0.0563 - accuracy: 0.9958 - val_loss: 0.9419 - val_accuracy: 0.8651\n",
            "Epoch 20/20\n",
            "49/49 [==============================] - 4s 78ms/step - loss: 0.0677 - accuracy: 0.9952 - val_loss: 0.9493 - val_accuracy: 0.8645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd71598cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxGjgaVpBvtS"
      },
      "source": [
        "Epoch 20/20\n",
        "49/49 [==============================] - 4s 79ms/step - loss: 0.0866 - accuracy: 0.9916 - val_loss: 1.2185 - val_accuracy: 0.8661\n",
        "\n",
        "At the 20th Epoch the accuracy of the model is 0.9916  and the loss is 0.0866 which means our model  is good  and highly accurate and the cross validation accuracy score of 0.8661 confirms the same "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "b_7QrvKfCTE7",
        "outputId": "0f2292fc-ddd2-4360-9285-ca35f19b6dc8"
      },
      "source": [
        "#Use the ‘yelp_labelled.txt’\n",
        "yelp_data = pd.read_csv('/content/yelp_labelled.txt', delimiter = \"\\t\")\n",
        "print('the number of rows and columns are',yelp_data.shape)\n",
        "yelp_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the number of rows and columns are (999, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wow... Loved this place.</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Now I am getting angry and I want my damn pho.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Wow... Loved this place.  1\n",
              "0                                 Crust is not good.  0\n",
              "1          Not tasty and the texture was just nasty.  0\n",
              "2  Stopped by during the late May bank holiday of...  1\n",
              "3  The selection on the menu was great and so wer...  1\n",
              "4     Now I am getting angry and I want my damn pho.  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrXSgAbYGxLw",
        "outputId": "33f0c829-fd31-47b5-90fb-e0f75c996889"
      },
      "source": [
        "#Use ‘sentence’ as feature and ‘label’ as target\n",
        "yelp_data.columns=['sentence', 'label']\n",
        "\n",
        "\n",
        "feature =yelp_data['sentence']\n",
        "target=yelp_data['label']\n",
        "\n",
        "#Tokenizer with 5,000 words\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#since the dision between test data and training data has not been specified i took the value 70/30 \n",
        "X_train,X_test,y_train,y_test=train_test_split(feature,target,test_size=0.3,random_state=42)\n",
        "print('the  size of the training data is',len(X_train))\n",
        "print('the  size of the testing  data is',len(X_test))\n",
        "\n",
        "#from keras.processing.text import Tokenizer\n",
        "# code refernce https://datascience.stackexchange.com/questions/37159/understanding-the-effect-of-num-words-of-tokenizer-in-keras\n",
        "yelp_token = Tokenizer(num_words = 5000)\n",
        "\n",
        "#code refence for transformation https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n",
        "\n",
        "yelp_token.fit_on_texts(X_train)\n",
        "yelp_token.fit_on_texts(X_test)\n",
        "\n",
        "#Pad the X_train and X_test values\n",
        "\n",
        "train_yelp_data=yelp_token.texts_to_sequences(X_train)\n",
        "test_yelp_data=yelp_token.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_len=len(yelp_token.word_index)+1\n",
        "#print(test_yelp_data)\n",
        "#padding syntax code refence https://realpython.com/python-keras-text-classification/\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_yelp = pad_sequences(train_yelp_data,padding='post', maxlen=100)\n",
        "X_test_yelp = pad_sequences(test_yelp_data,padding='post', maxlen=100)\n",
        "print(y_train)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)\n",
        "print(X_train_yelp)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the  size of the training data is 699\n",
            "the  size of the testing  data is 300\n",
            "728    1\n",
            "630    1\n",
            "394    1\n",
            "777    0\n",
            "598    0\n",
            "      ..\n",
            "106    1\n",
            "270    1\n",
            "860    1\n",
            "435    1\n",
            "102    1\n",
            "Name: label, Length: 699, dtype: int64\n",
            "[[ 43  13   1 ...   0   0   0]\n",
            " [  1 585 293 ...   0   0   0]\n",
            " [  6 898   1 ...   0   0   0]\n",
            " ...\n",
            " [332 104  23 ...   0   0   0]\n",
            " [ 85  42 160 ...   0   0   0]\n",
            " [  3 183  48 ...   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0dKzBgb4jx4",
        "outputId": "2a836cda-55ed-487b-b8cf-138a86f846f0"
      },
      "source": [
        "#Use a ‘Sequential()’ model with \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPool1D\n",
        "\n",
        "epochs = 20\n",
        "maxlen = 100\n",
        "embedding_dim = 50\n",
        "num_filters = 64\n",
        "kernel_size = 5\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "#embedding code refrence https://colab.research.google.com/drive/1B6TLosBa-q9d9knwlRkvGokv2W2MGtsH#scrollTo=jVzZ9uKzoY-h\n",
        "# in the embedding layer the input_dim is \n",
        "\n",
        "yelp_model=Sequential()\n",
        "yelp_model.add(Embedding(input_dim=vocab_len,output_dim=embedding_dim,input_length = maxlen) )\n",
        "yelp_model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu'))\n",
        "yelp_model.add(GlobalMaxPool1D())\n",
        "yelp_model.add(Dense(10, activation = \"relu\"))\n",
        "yelp_model.add(Dense(1, activation = \"relu\"))\n",
        "\n",
        "yelp_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 50)           103600    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 96, 64)            16064     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 120,325\n",
            "Trainable params: 120,325\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8BCJLNwIVJ3",
        "outputId": "68576aac-b63d-48dc-a41f-dc6bf6996b21"
      },
      "source": [
        "#using the default model vocab_len\n",
        "#Determine the loss, accuracy of the training and testing model\n",
        "#Use a ‘Sequential()’ model with \n",
        "\n",
        "yelp_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "yelp_model.fit(X_train_yelp,y_train,epochs=20,batch_size=32,validation_data=(X_test_yelp,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "22/22 [==============================] - 1s 29ms/step - loss: 1.4416 - accuracy: 0.4925 - val_loss: 0.6873 - val_accuracy: 0.5267\n",
            "Epoch 2/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.6621 - accuracy: 0.6251 - val_loss: 0.6890 - val_accuracy: 0.5133\n",
            "Epoch 3/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.6109 - accuracy: 0.8012 - val_loss: 0.6726 - val_accuracy: 0.6167\n",
            "Epoch 4/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.5642 - accuracy: 0.9494 - val_loss: 0.6592 - val_accuracy: 0.6267\n",
            "Epoch 5/20\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.4944 - accuracy: 0.9316 - val_loss: 0.6338 - val_accuracy: 0.6467\n",
            "Epoch 6/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.4075 - accuracy: 0.9447 - val_loss: 0.5860 - val_accuracy: 0.6967\n",
            "Epoch 7/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.2811 - accuracy: 0.9510 - val_loss: 0.5324 - val_accuracy: 0.7333\n",
            "Epoch 8/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.1360 - accuracy: 0.9650 - val_loss: 0.4741 - val_accuracy: 0.7500\n",
            "Epoch 9/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.0608 - accuracy: 0.9882 - val_loss: 0.4548 - val_accuracy: 0.7800\n",
            "Epoch 10/20\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.0265 - accuracy: 0.9992 - val_loss: 0.5079 - val_accuracy: 0.8033\n",
            "Epoch 11/20\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.0116 - accuracy: 0.9997 - val_loss: 0.5450 - val_accuracy: 0.7967\n",
            "Epoch 12/20\n",
            "22/22 [==============================] - 0s 22ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.5475 - val_accuracy: 0.7967\n",
            "Epoch 13/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5621 - val_accuracy: 0.7967\n",
            "Epoch 14/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5528 - val_accuracy: 0.7967\n",
            "Epoch 15/20\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5518 - val_accuracy: 0.7967\n",
            "Epoch 16/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 2.3215e-04 - accuracy: 1.0000 - val_loss: 0.5482 - val_accuracy: 0.8000\n",
            "Epoch 17/20\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 8.4931e-05 - accuracy: 1.0000 - val_loss: 0.5506 - val_accuracy: 0.7967\n",
            "Epoch 18/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 1.6239e-04 - accuracy: 1.0000 - val_loss: 0.5755 - val_accuracy: 0.7967\n",
            "Epoch 19/20\n",
            "22/22 [==============================] - 0s 20ms/step - loss: 4.6609e-06 - accuracy: 1.0000 - val_loss: 0.5677 - val_accuracy: 0.8000\n",
            "Epoch 20/20\n",
            "22/22 [==============================] - 0s 21ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.5994 - val_accuracy: 0.7967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd61cc11d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U40h0OM1U1x3"
      },
      "source": [
        "#the training accuracy of the model is 100% and the loss is 0% but the validation accuracy score is 79% and loss is 59% which means the model is a good model but can be enhanced further"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-BBZBzKTh9M",
        "outputId": "1c72d04d-5d41-42bc-a907-c5fa007ddff1"
      },
      "source": [
        "yelp_model.evaluate(X_test_yelp,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 4ms/step - loss: 0.5994 - accuracy: 0.7967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5993621349334717, 0.79666668176651]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}